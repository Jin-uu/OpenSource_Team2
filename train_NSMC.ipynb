{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "980f20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_data_path': './ratings_train.txt',\n",
    "    'val_data_path': './ratings_test.txt',\n",
    "    'save_path': './model3',\n",
    "    'max_epochs': 1,\n",
    "    'model_path': 'beomi/KcELECTRA-base',\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 5e-5,\n",
    "    'warmup_ratio': 0.0,\n",
    "    'max_seq_len': 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c822def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length):\n",
    "        df = pd.read_csv(csv_file, sep='\\t')\n",
    "        # NaN 값 제거\n",
    "        df = df.dropna(axis=0)\n",
    "        # 중복 제거\n",
    "        df.drop_duplicates(subset=['document'], inplace=True)\n",
    "        self.input_ids = tokenizer.batch_encode_plus(\n",
    "            df['document'].to_list(),\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=False,\n",
    "            truncation=True,\n",
    "        )['input_ids']\n",
    "        self.labels = torch.LongTensor(df['label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af73225",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed43c3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ElectraTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ElectraTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForSequenceClassification, ElectraTokenizerFast\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(args['model_path'])\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(args['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0d47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, train_dataloader, args):\n",
    "    model.train()\n",
    "    model.to('cuda')\n",
    "    global_total_step = len(train_dataloader) * args['max_epochs']\n",
    "    global_step = 0\n",
    "    optimizer = AdamW(model.parameters(), lr=args['learning_rate'], weight_decay=0.0)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=global_total_step)\n",
    "    with tqdm(total=global_total_step, unit='step') as t:\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for epoch in range(args['max_epochs']):\n",
    "            for batch in train_dataloader:\n",
    "                global_step += 1\n",
    "                b_input_ids = batch[0].to('cuda', non_blocking=True)\n",
    "                b_labels = batch[1].to('cuda', non_blocking=True)\n",
    "                model.zero_grad(set_to_none=True)\n",
    "                outputs = model(\n",
    "                    input_ids=b_input_ids,\n",
    "                    labels=b_labels\n",
    "                )\n",
    "                loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                preds = logits.detach().argmax(dim=-1).cpu().numpy()\n",
    "                out_label_ids = b_labels.detach().cpu().numpy()\n",
    "                total_correct += (preds == out_label_ids).sum()\n",
    "\n",
    "                batch_loss = loss.item() * len(b_input_ids)\n",
    "\n",
    "                total += len(b_input_ids)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                t.set_postfix(loss='{:.6f}'.format(batch_loss),\n",
    "                              accuracy='{:.2f}'.format(total_correct / total * 100))\n",
    "                t.update(1)\n",
    "                del b_input_ids\n",
    "                del outputs\n",
    "                del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f32f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SUNHO9~1\\AppData\\Local\\Temp/ipykernel_11256/1225759754.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_data_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNSMCDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_data_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_seq_len'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m train_data_loader = DataLoader(\n\u001b[0;32m      5\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SUNHO9~1\\AppData\\Local\\Temp/ipykernel_11256/2298293920.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, csv_file, tokenizer, max_length)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# 중복 제거\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         self.input_ids = tokenizer.batch_encode_plus(\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\sunho9889\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2644\u001b[0m         )\n\u001b[0;32m   2645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\sunho9889\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;31m#                    ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m         tokens_and_encodings = [\n\u001b[0m\u001b[0;32m    438\u001b[0m             self._convert_encoding(\n\u001b[0;32m    439\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\sunho9889\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         tokens_and_encodings = [\n\u001b[1;32m--> 438\u001b[1;33m             self._convert_encoding(\n\u001b[0m\u001b[0;32m    439\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mreturn_token_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_token_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_set = NSMCDataset(args['train_data_path'], tokenizer, args['max_seq_len'])\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_data_set,\n",
    "    batch_size=args['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be282ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_data_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb47b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(args['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "678f9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평점 10\n",
    "pos_text = '이방원을 다룬 드라마중 최고였다고 자부함. 진짜 이방원을 보여준 듯이 연기와 인물묘사나 주변상황이 재밌었고 스토리도 진부하지 않았음. 다시 이런드라마를 볼수 있을지~ 진짜 이런 드라마하나 또 나왔음 함.'\n",
    "# 평점 0\n",
    "neg_text = '핵노잼 후기보고 낙였네 방금보고왔는데 개실망 재미없어요'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0086df68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이방원을 다룬 드라마중 최고였다고 자부함. 진짜 이방원을 보여준 듯이 연기와 인물묘사나 주변상황이 재밌었고 스토리도 진부하지 않았음. 다시 이런드라마를 볼수 있을지~ 진짜 이런 드라마하나 또 나왔음 함. : 1\n",
      "핵노잼 후기보고 낙였네 방금보고왔는데 개실망 재미없어요 : 0\n"
     ]
    }
   ],
   "source": [
    "pos_input_vector = tokenizer.encode(pos_text, return_tensors='pt').to('cuda')\n",
    "pos_pred = model(input_ids=pos_input_vector, labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{pos_text} : {pos_pred[0]}')\n",
    "\n",
    "neg_input_vector = tokenizer.encode(neg_text, return_tensors='pt').to('cuda')\n",
    "neg_pred = model(input_ids=neg_input_vector, labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{neg_text} : {neg_pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4101517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: D:\\sunho9889\\anaconda\n",
      "\n",
      "  added / updated specs:\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.12.0               |   py39hcbf5309_0         1.0 MB  conda-forge\n",
      "    huggingface_hub-0.6.0      |     pyhd8ed1ab_0          64 KB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    sacremoses-0.0.53          |     pyhd8ed1ab_0         427 KB  conda-forge\n",
      "    tokenizers-0.12.1          |   py39h8c9392a_0         3.0 MB  conda-forge\n",
      "    transformers-4.18.0        |   py39haa95532_0         2.9 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         7.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  huggingface_hub    conda-forge/noarch::huggingface_hub-0.6.0-pyhd8ed1ab_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.9-2_cp39\n",
      "  sacremoses         conda-forge/noarch::sacremoses-0.0.53-pyhd8ed1ab_0\n",
      "  tokenizers         conda-forge/win-64::tokenizers-0.12.1-py39h8c9392a_0\n",
      "  transformers       pkgs/main/win-64::transformers-4.18.0-py39haa95532_0\n",
      "  typing-extensions  pkgs/main/noarch::typing-extensions-3.10.0.2-hd3eb1b0_0\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              pkgs/main::conda-4.12.0-py39haa95532_0 --> conda-forge::conda-4.12.0-py39hcbf5309_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "sacremoses-0.0.53    | 427 KB    |            |   0% \n",
      "sacremoses-0.0.53    | 427 KB    | ########## | 100% \n",
      "sacremoses-0.0.53    | 427 KB    | ########## | 100% \n",
      "\n",
      "conda-4.12.0         | 1.0 MB    |            |   0% \n",
      "conda-4.12.0         | 1.0 MB    | ########## | 100% \n",
      "conda-4.12.0         | 1.0 MB    | ########## | 100% \n",
      "\n",
      "tokenizers-0.12.1    | 3.0 MB    |            |   0% \n",
      "tokenizers-0.12.1    | 3.0 MB    | ########## | 100% \n",
      "tokenizers-0.12.1    | 3.0 MB    | ########## | 100% \n",
      "\n",
      "huggingface_hub-0.6. | 64 KB     |            |   0% \n",
      "huggingface_hub-0.6. | 64 KB     | ##5        |  25% \n",
      "huggingface_hub-0.6. | 64 KB     | ########## | 100% \n",
      "\n",
      "python_abi-3.9       | 4 KB      |            |   0% \n",
      "python_abi-3.9       | 4 KB      | ########## | 100% \n",
      "\n",
      "transformers-4.18.0  | 2.9 MB    |            |   0% \n",
      "transformers-4.18.0  | 2.9 MB    | ########## | 100% \n",
      "transformers-4.18.0  | 2.9 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "126ba04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연기 정말 멋졌어요!! 어쩐지~ 뭘 믿고 입안에 저 공구를 넣게 해줬나 생각했는데 의사가 한거였군요 ㅋㅋㅋㅋ : 1\n"
     ]
    }
   ],
   "source": [
    "input_comment = '연기 정말 멋졌어요!! 어쩐지~ 뭘 믿고 입안에 저 공구를 넣게 해줬나 생각했는데 의사가 한거였군요 ㅋㅋㅋㅋ'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1d5caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제니가 없었다면 지금시대 여성들은 멀 입고 다녓을까 궁금함 : 1\n"
     ]
    }
   ],
   "source": [
    "input_comment = '제니가 없었다면 지금시대 여성들은 멀 입고 다녓을까 궁금함'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e808100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 저러고 사냐 보는내가 다 쪽팔리네 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 현웃터진다 : 0\n"
     ]
    }
   ],
   "source": [
    "input_comment = '왜 저러고 사냐 보는내가 다 쪽팔리네 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 현웃터진다'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bea5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제니짝퉁ㅋㅋ 약간 치즈인더트랩보느거같음 : 0\n"
     ]
    }
   ],
   "source": [
    "input_comment = '제니짝퉁ㅋㅋ 약간 치즈인더트랩보느거같음'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "566a2e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:53 그녀는 제니처럼되기 위해 너무 열심히 노력하고 있습니다 ㅋㅋㅋㅋㅋ : 1\n"
     ]
    }
   ],
   "source": [
    "input_comment = '3:53 그녀는 제니처럼되기 위해 너무 열심히 노력하고 있습니다 ㅋㅋㅋㅋㅋ'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bed3d4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "치료하는게 너무 리얼하다 싶었는데 진짜 치과의사님 손이었다니ㅋㅋㅋ : 1\n"
     ]
    }
   ],
   "source": [
    "input_comment = '치료하는게 너무 리얼하다 싶었는데 진짜 치과의사님 손이었다니ㅋㅋㅋ'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "166aff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하나만 하세요 얼굴까지 잘생기셨우... : 0\n"
     ]
    }
   ],
   "source": [
    "input_comment = '하나만 하세요 얼굴까지 잘생기셨우...'\n",
    "input_vector = tokenizer.encode(input_comment, return_tensors='pt').to('cuda')\n",
    "pred = model(input_ids=input_vector,labels=None).logits.argmax(dim=-1).tolist()\n",
    "print(f'{input_comment} : {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d1718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
